{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Contest number</th>\n",
       "      <th>Word</th>\n",
       "      <th>Number of  reported results</th>\n",
       "      <th>Number in hard mode</th>\n",
       "      <th>1 try</th>\n",
       "      <th>2 tries</th>\n",
       "      <th>3 tries</th>\n",
       "      <th>4 tries</th>\n",
       "      <th>5 tries</th>\n",
       "      <th>6 tries</th>\n",
       "      <th>7 or more tries (X)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-31 00:00:00</td>\n",
       "      <td>560</td>\n",
       "      <td>manly</td>\n",
       "      <td>20380</td>\n",
       "      <td>1899</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-30 00:00:00</td>\n",
       "      <td>559</td>\n",
       "      <td>molar</td>\n",
       "      <td>21204</td>\n",
       "      <td>1973</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-29 00:00:00</td>\n",
       "      <td>558</td>\n",
       "      <td>havoc</td>\n",
       "      <td>20001</td>\n",
       "      <td>1919</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-28 00:00:00</td>\n",
       "      <td>557</td>\n",
       "      <td>impel</td>\n",
       "      <td>20160</td>\n",
       "      <td>1937</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-12-27 00:00:00</td>\n",
       "      <td>556</td>\n",
       "      <td>condo</td>\n",
       "      <td>20879</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date Contest number   Word Number of  reported results  \\\n",
       "1  2022-12-31 00:00:00            560  manly                       20380   \n",
       "2  2022-12-30 00:00:00            559  molar                       21204   \n",
       "3  2022-12-29 00:00:00            558  havoc                       20001   \n",
       "4  2022-12-28 00:00:00            557  impel                       20160   \n",
       "5  2022-12-27 00:00:00            556  condo                       20879   \n",
       "\n",
       "  Number in hard mode 1 try 2 tries 3 tries 4 tries 5 tries 6 tries  \\\n",
       "1                1899     0       2      17      37      29      12   \n",
       "2                1973     0       4      21      38      26       9   \n",
       "3                1919     0       2      16      38      30      12   \n",
       "4                1937     0       3      21      40      25       9   \n",
       "5                2012     0       2      17      35      29      14   \n",
       "\n",
       "  7 or more tries (X)  \n",
       "1                   2  \n",
       "2                   1  \n",
       "3                   2  \n",
       "4                   1  \n",
       "5                   3  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ori_data_path = r'D:\\美赛\\比赛ing\\Code\\Problem_C_Data_Wordle.xlsx'\n",
    "ori_data = pd.read_excel(ori_data_path)\n",
    "Column_name = ori_data.iloc[0,:].values\n",
    "ori_data.columns = Column_name\n",
    "ori_data = ori_data.drop(axis=0,index=0)\n",
    "ori_data = ori_data.iloc[:,1:]\n",
    "ori_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>count</th>\n",
       "      <th>noun</th>\n",
       "      <th>verb</th>\n",
       "      <th>adj</th>\n",
       "      <th>adv</th>\n",
       "      <th>pron</th>\n",
       "      <th>num</th>\n",
       "      <th>prep</th>\n",
       "      <th>entropy</th>\n",
       "      <th>...</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>w</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>vowel_num</th>\n",
       "      <th>total_Word_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>manly</td>\n",
       "      <td>-0.287384</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.386198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>molar</td>\n",
       "      <td>-0.294364</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.698246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>havoc</td>\n",
       "      <td>-0.282446</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.247338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>impel</td>\n",
       "      <td>-0.300813</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.649583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>condo</td>\n",
       "      <td>-0.228084</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.383127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Word     count  noun  verb  adj  adv  pron  num  prep   entropy  ...    s  \\\n",
       "0  manly -0.287384     0     0    1    1     0    0     0  0.386198  ...  0.0   \n",
       "1  molar -0.294364     1     0    1    0     0    0     0  0.698246  ...  0.0   \n",
       "2  havoc -0.282446     1     1    0    0     0    0     0  0.247338  ...  0.0   \n",
       "3  impel -0.300813     0     1    0    0     0    0     0  1.649583  ...  0.0   \n",
       "4  condo -0.228084     1     0    0    0     0    0     0 -0.383127  ...  0.0   \n",
       "\n",
       "     t    u      v    w    x     y    z  vowel_num  total_Word_class  \n",
       "0  0.0  0.0  0.000  0.0  0.0  1.97  0.0          1                 2  \n",
       "1  0.0  0.0  0.000  0.0  0.0  0.00  0.0          2                 2  \n",
       "2  0.0  0.0  0.978  0.0  0.0  0.00  0.0          2                 2  \n",
       "3  0.0  0.0  0.000  0.0  0.0  0.00  0.0          2                 1  \n",
       "4  0.0  0.0  0.000  0.0  0.0  0.00  0.0          2                 1  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_path = r'D:\\美赛\\比赛ing\\Code\\data_attr_no_pos.csv'\n",
    "attr = pd.read_csv(attr_path)\n",
    "\n",
    "attr = attr.drop('Unnamed: 0',axis=1)\n",
    "attr = attr.drop('Contest number',axis=1)\n",
    "\n",
    "count_mean = attr.iloc[:,1].mean()\n",
    "count_std = attr.iloc[:,1].std()\n",
    "attr['count'] = (attr['count'] - count_mean)/count_std\n",
    "\n",
    "en_mean = attr.iloc[:,9].mean()\n",
    "en_std = attr.iloc[:,9].std()\n",
    "attr['entropy'] = (attr['entropy'] - en_mean)/en_std\n",
    "\n",
    "attr.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word_to_vector function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.02, 5.99, 8.17, 9.06, 12.7]\n"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "for i in range(359):\n",
    "    word = attr.iloc[i,0]\n",
    "    word = word.strip()\n",
    "    word_dict[word] = []\n",
    "    for letter in list(word):\n",
    "        feature = attr.loc[i,letter]\n",
    "        # print(i,letter,word)\n",
    "        word_dict[word].append(feature)\n",
    "print(word_dict['grate'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ori_data['Number in hard mode']/ori_data.iloc[:,3]\n",
    "target = pd.DataFrame(target) \n",
    "target.columns = ['Percentage']\n",
    "target = target.iloc[::-1]\n",
    "target.reset_index(drop=True,inplace=True)\n",
    "target = target * 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the training data as [ wordvector, total_Word_class, vowel_num, count, entropy, percentage ] with 10 input and 7 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.02, 5.99, 8.17, 9.06, 12.7, 2, 2, -0.2941134695897084, 0.6826346903801769, 6.555838481125617, 1, 14, 29, 28, 16, 8, 3]\n",
      "356\n"
     ]
    }
   ],
   "source": [
    "train_data_1 = {}\n",
    "for i in range(359):\n",
    "    if sum(ori_data.iloc[i,-7:]) > 105 or sum(ori_data.iloc[i,-7:]) < 95:\n",
    "        continue\n",
    "    word = attr.iloc[i,0].strip()\n",
    "    if len(list(word)) != 5:\n",
    "        continue\n",
    "    # print(word)\n",
    "    train_data_1[word] = word_dict[word]\n",
    "    \n",
    "    train_data_1[word].append(attr.loc[i,'total_Word_class']) \n",
    "    train_data_1[word].append(attr.loc[i,'vowel_num']) \n",
    "    train_data_1[word].append(attr.loc[i,'count']) \n",
    "    train_data_1[word].append(attr.loc[i,'entropy']) \n",
    "\n",
    "    train_data_1[word].append(target.iloc[i,:].values[0]) \n",
    "\n",
    "    train_data_1[word].append(ori_data.iloc[i,-7])\n",
    "    # print(attr.iloc[i,-7])\n",
    "    train_data_1[word].append(ori_data.iloc[i,-6]) \n",
    "    # print(attr.iloc[i,-6])\n",
    "    train_data_1[word].append(ori_data.iloc[i,-5]) \n",
    "    train_data_1[word].append(ori_data.iloc[i,-4]) \n",
    "    train_data_1[word].append(ori_data.iloc[i,-3]) \n",
    "    train_data_1[word].append(ori_data.iloc[i,-2]) \n",
    "    train_data_1[word].append(ori_data.iloc[i,-1]) \n",
    "\n",
    "print(train_data_1['grate'])\n",
    "print(len(train_data_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356\n"
     ]
    }
   ],
   "source": [
    "valid_key = []\n",
    "for key in train_data_1.keys():\n",
    "    valid_key.append(key)\n",
    "print(len(valid_key))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data processing, we build the deep learing model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About hyperparameters\n",
    "seq_len = 10            # 每个样本包含的序列长度\n",
    "tar_len = 7             # 准备通过样本进行多个预测\n",
    "n_features = seq_len    # 输入的特征数，实际上为10\n",
    "n_hidden = 32           # hidden_size,也就是d_model\n",
    "n_layers = 2            # 层数\n",
    "n_heads = 4             # 注意力头数\n",
    "dropout = 0.5           # 随即失活数量\n",
    "n_epochs = 200          # 训练轮数\n",
    "position_len = 1000     # 最长序列长度\n",
    "\n",
    "# define the dataset about time series\n",
    "class DistributionDataset(Dataset):\n",
    "    def __init__(self, data, seq_len,tar_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.tar_len = tar_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.data) == 72:\n",
    "            word = valid_key[idx + 284]\n",
    "        # print(word)\n",
    "        else:\n",
    "            word = valid_key[idx]\n",
    "        return self.data[word][:self.seq_len], self.data[word][self.seq_len:]\n",
    "\n",
    "# 网上调整函数\n",
    "def collate_fn(batch):\n",
    "    # batch是一个列表，其中是一个一个的元组，每个元组是dataset中_getitem__的结果\n",
    "    batch = list(zip(*batch))\n",
    "    train_data = batch[0]\n",
    "    label = batch[1]\n",
    "    # print(label)\n",
    "    # label = label.astype(np.int32)\n",
    "    # time_sery = time_sery.astype(np.int32)\n",
    "    train_data = torch.tensor(train_data,dtype=torch.float32)\n",
    "    label = torch.tensor(label,dtype=torch.float32)\n",
    "    # print(label.shape)\n",
    "    del batch\n",
    "    return  train_data, label\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=position_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_layers, n_heads, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(n_features, n_hidden)\n",
    "        # self.position_encoding = PositionalEncoding(n_hidden)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(n_hidden, n_heads, 2048, dropout=dropout)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, n_layers)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(n_hidden, n_heads,dim_feedforward=2048, dropout=dropout)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, n_layers)\n",
    "        self.fc = nn.Linear(n_hidden, tar_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        # print(x.shape)\n",
    "        x = x.unsqueeze(1) \n",
    "        # print(x.shape)\n",
    "        # x = torch.transpose(x)\n",
    "        # print(x.shape)\n",
    "        x = self.embedding(x)\n",
    "        # print(x.shape)\n",
    "        # x = self.position_encoding(x)\n",
    "        # print(x.shape)\n",
    "        x = self.encoder(x)\n",
    "        # print(x.shape)\n",
    "        x = self.decoder(x, x)\n",
    "        # print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        # print(x.shape)\n",
    "        x = x.squeeze(1)        # [batch_size, target_len]\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # print(len(inputs))\n",
    "        inputs = torch.stack([torch.Tensor(i).to(device) for i in inputs])\n",
    "        targets = torch.stack([torch.Tensor(i).to(torch.float32).to(device) for i in targets])\n",
    "        # print(targets.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).to(torch.float32)\n",
    "        # print(outputs.shape)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    return train_loss\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = torch.stack([torch.Tensor(i).to(device) for i in inputs])\n",
    "            targets = torch.stack([torch.Tensor(i).to(device) for i in targets])\n",
    "            outputs = model(inputs).float()\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    return val_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formorlise the data\n",
    "data = train_data_1\n",
    "train_size = int(len(data) * 0.8)\n",
    "train_data = {}\n",
    "for idx, item in enumerate(train_data_1):\n",
    "    if idx + 1 == train_size:\n",
    "        train_data[item] = []\n",
    "        train_data[item] += (train_data_1[item])\n",
    "        break\n",
    "    else:\n",
    "        train_data[item] = []\n",
    "        train_data[item] += (train_data_1[item])\n",
    "\n",
    "# print(len(train_data))\n",
    "val_data = {}\n",
    "for idx, item in enumerate(train_data_1):\n",
    "    if idx + 1 <= train_size:\n",
    "        continue\n",
    "    else:\n",
    "        val_data[item] = []\n",
    "        val_data[item] += (train_data_1[item])\n",
    "# print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Train Loss: 332.3934, Val Loss: 293.5478\n",
      "Epoch 11/500, Train Loss: 142.0074, Val Loss: 111.3002\n",
      "Epoch 21/500, Train Loss: 43.4047, Val Loss: 28.5123\n",
      "Epoch 31/500, Train Loss: 30.2454, Val Loss: 21.3998\n",
      "Epoch 41/500, Train Loss: 28.9113, Val Loss: 21.3801\n",
      "Epoch 51/500, Train Loss: 27.9206, Val Loss: 21.0622\n",
      "Epoch 61/500, Train Loss: 27.5036, Val Loss: 21.9375\n",
      "Epoch 71/500, Train Loss: 25.2580, Val Loss: 23.5311\n",
      "Epoch 81/500, Train Loss: 26.0653, Val Loss: 22.7093\n",
      "Epoch 91/500, Train Loss: 24.2599, Val Loss: 22.8858\n",
      "Epoch 101/500, Train Loss: 24.2713, Val Loss: 22.3048\n",
      "Epoch 111/500, Train Loss: 23.0956, Val Loss: 23.2071\n",
      "Epoch 121/500, Train Loss: 23.7813, Val Loss: 21.4477\n",
      "Epoch 131/500, Train Loss: 23.5555, Val Loss: 25.1860\n",
      "Epoch 141/500, Train Loss: 22.5946, Val Loss: 22.1910\n",
      "Epoch 151/500, Train Loss: 22.4300, Val Loss: 22.0930\n",
      "Epoch 161/500, Train Loss: 23.6995, Val Loss: 22.9584\n",
      "Epoch 171/500, Train Loss: 21.3826, Val Loss: 25.0371\n",
      "Epoch 181/500, Train Loss: 22.0359, Val Loss: 24.5051\n",
      "Epoch 191/500, Train Loss: 21.2659, Val Loss: 24.9704\n",
      "Epoch 201/500, Train Loss: 20.3414, Val Loss: 23.6569\n",
      "Epoch 211/500, Train Loss: 19.7385, Val Loss: 26.8158\n",
      "Epoch 221/500, Train Loss: 19.9000, Val Loss: 23.6831\n",
      "Epoch 231/500, Train Loss: 19.8220, Val Loss: 26.4599\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [195], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[0;32m     21\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_loader, optimizer, criterion, device)\n\u001b[1;32m---> 22\u001b[0m     val_loss \u001b[39m=\u001b[39m evaluate(model, val_loader, criterion, device)\n\u001b[0;32m     23\u001b[0m     \u001b[39m# if val_loss <= 20 and epoch > 100: \u001b[39;00m\n\u001b[0;32m     24\u001b[0m         \u001b[39m# break\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     val_loss_compare \u001b[39m=\u001b[39m val_loss\n",
      "Cell \u001b[1;32mIn [194], line 118\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, val_loader, criterion, device)\u001b[0m\n\u001b[0;32m    116\u001b[0m inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([torch\u001b[39m.\u001b[39mTensor(i)\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m inputs])\n\u001b[0;32m    117\u001b[0m targets \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([torch\u001b[39m.\u001b[39mTensor(i)\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m targets])\n\u001b[1;32m--> 118\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m    119\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m    120\u001b[0m val_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32md:\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [194], line 85\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     83\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x)\n\u001b[0;32m     84\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(x, x)\n\u001b[0;32m     86\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m     87\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)\n",
      "File \u001b[1;32md:\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:333\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    330\u001b[0m output \u001b[39m=\u001b[39m tgt\n\u001b[0;32m    332\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 333\u001b[0m     output \u001b[39m=\u001b[39m mod(output, memory, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask,\n\u001b[0;32m    334\u001b[0m                  memory_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[0;32m    335\u001b[0m                  tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask,\n\u001b[0;32m    336\u001b[0m                  memory_key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask)\n\u001b[0;32m    338\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    339\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32md:\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:652\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    651\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask))\n\u001b[1;32m--> 652\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mha_block(x, memory, memory_mask, memory_key_padding_mask))\n\u001b[0;32m    653\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[0;32m    655\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:669\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._mha_block\u001b[1;34m(self, x, mem, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mha_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor, mem: Tensor,\n\u001b[0;32m    668\u001b[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 669\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultihead_attn(x, mem, mem,\n\u001b[0;32m    670\u001b[0m                             attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    671\u001b[0m                             key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    672\u001b[0m                             need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    673\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(x)\n",
      "File \u001b[1;32md:\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1167\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[0;32m   1156\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1157\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m   1158\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1164\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[0;32m   1165\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[0;32m   1166\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[0;32m   1168\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m   1169\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m   1170\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m   1171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m   1172\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m   1173\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[0;32m   1174\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[0;32m   1175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m   1176\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32md:\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:5167\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[0;32m   5163\u001b[0m     attn_output_weights \u001b[39m=\u001b[39m dropout(attn_output_weights, p\u001b[39m=\u001b[39mdropout_p)\n\u001b[0;32m   5165\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(attn_output_weights, v)\n\u001b[1;32m-> 5167\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39;49mtranspose(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(tgt_len \u001b[39m*\u001b[39m bsz, embed_dim)\n\u001b[0;32m   5168\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n\u001b[0;32m   5169\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mview(tgt_len, bsz, attn_output\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create the dataset and loader\n",
    "train_dataset = DistributionDataset(train_data, seq_len,tar_len)\n",
    "val_dataset = DistributionDataset(val_data, seq_len,tar_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16,collate_fn=collate_fn)\n",
    "\n",
    "# Initialise the model and optimizer\n",
    "model = TransformerModel(n_features=n_features, n_hidden=n_hidden, n_layers=n_layers, n_heads=n_heads, dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 训练模型\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "val_loss_compare = 0\n",
    "count = 0\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    # if val_loss <= 20 and epoch > 100: \n",
    "        # break\n",
    "    val_loss_compare = val_loss\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "# plot train_loss and val_loss\n",
    "plt.plot(val_loss_list,label='val')\n",
    "plt.plot(train_loss_list,label='train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.541527938842773"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(val_loss_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, pre_data, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_input = torch.Tensor(pre_data).unsqueeze(0).to(device)\n",
    "        # print(len(pre_data))\n",
    "        test_output = model(test_input).cpu().numpy()[0]\n",
    "        # print(test_output.cpu().numpy())\n",
    "        return test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 55 17\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.3\n",
    "perfect = 0\n",
    "good = 0\n",
    "fail = 0\n",
    "for idx in range(len(val_data)):\n",
    "    score = 0\n",
    "    word = valid_key[idx + 284]\n",
    "    pred = prediction(model,train_data_1[word][:10],device)\n",
    "    truth = train_data_1[word][10:]\n",
    "    for j in range(7):\n",
    "        if abs(pred[j] - truth[j])/(truth[j]+0.0000001) < threshold:\n",
    "            score += 1\n",
    "    if score >= 3:\n",
    "        good += 1\n",
    "        if score >= 5:\n",
    "            perfect += 1\n",
    "    else:\n",
    "        fail += 1\n",
    "\n",
    "print(perfect, good, fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: [0.7947500348091125, 8.904172897338867, 28.913963317871094, 35.367122650146484, 21.61078453063965, 9.220895767211914, 2.0617473125457764]\n",
      "truth: [1, 8, 33, 34, 17, 7, 1]\n"
     ]
    }
   ],
   "source": [
    "print('pred:', prediction(model,train_data_1['cloth'][:10],device).tolist())\n",
    "print('truth:', train_data_1['cloth'][10:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4e4cda6f83d04b07e464991ef8b8868b432cebb831b436ee9ba4d4dbf632117"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
